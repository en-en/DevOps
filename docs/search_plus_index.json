{"./":{"url":"./","title":"序言","keywords":"","body":"DevOps Handbook——DevOps 应用架构实践手册 Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎，Google 将其作为初始和核心项目贡献给 CNCF（云原生计算基金会），近年来逐渐发展出了云原生生态。 Kubernetes 的目标不仅仅是一个编排系统，而是提供一个规范用以描述集群的架构，定义服务的最终状态，使系统自动地达到和维持该状态。Kubernetes 作为云原生应用的基石，相当于一个云原生操作系统，其重要性不言而喻。 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括 容器、服务网格、微服务、不可变基础设施 和 声明式 API。这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。——CNCF（云原生计算基金会）。 主题 云原生开源组件 云原生应用与微服务架构 基于 Kubernetes 运维架构 公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:58:04 "},"cloud-native/cloud-native-definition.html":{"url":"cloud-native/cloud-native-definition.html","title":"云原生（Cloud Native）的定义","keywords":"","body":"云原生（Cloud Native）的定义 Pivotal最初的定义 符合12因素应用 面向微服务架构 自服务敏捷架构 基于API的协作 抗脆弱性 CNCF最初的定义 到了2015年Google主导成立了云原生计算基金会（CNCF），起初CNCF对云原生（Cloud Native）的定义包含以下三个方面： 应用容器化 面向微服务架构 应用支持容器的编排调度 重定义 到了2018年，随着近几年来云原生生态的不断壮大，所有主流云计算供应商都加入了该基金会，且从Cloud Native Landscape中可以看出云原生有意蚕食原先非云原生应用的部分。CNCF基金会中的会员以及容纳的项目越来越多，该定义已经限制了云原生生态的发展，CNCF为云原生进行了重新定位。 以下是CNCF对云原生的重新定义 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。 公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:05 "},"cloud-native/cloud-native-philosophy.html":{"url":"cloud-native/cloud-native-philosophy.html","title":"云原生的设计","keywords":"","body":"云原生的设计哲学 云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。 云原生的设计理念 云原生系统的设计理念如下: 面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密； 云原生应用程序 云原生应用程序被设计为在平台上运行，并设计用于弹性，敏捷性，可操作性和可观察性。弹性包含失败而不是试图阻止它们；它利用了在平台上运行的动态特性。敏捷性允许快速部署和快速迭代。可操作性从应用程序内部控制应用程序生命周期，而不是依赖外部进程和监视器。可观察性提供信息来回答有关应用程序状态的问题。 云原生定义 云原生应用程序通过各种方法获取这些特征。它通常取决于应用程序的运行位置以及企业流程和文化。以下是实现云原生应用程序所需特性的常用方法： 微服务 健康报告 遥测数据 弹性 声明式的，而不是命令式的 微服务 作为单个实体进行管理和部署的应用程序通常称为单体应用。最初开发应用程序时，单体有很多好处。它们更易于理解，并允许您在不影响其他服务的情况下更改主要功能。 随着应用程序复杂性的增长，单体应用的益处逐渐减少。它们变得更难理解，而且失去了敏捷性，因为工程师很难推断和修改代码。 对付复杂性的最好方法之一是将明确定义的功能分成更小的服务，并让每个服务独立迭代。这增加了应用程序的灵活性，允许根据需要更轻松地更改部分应用程序。每个微服务可以由单独的团队进行管理，使用适当的语言编写，并根据需要进行独立扩缩容。 只要每项服务都遵守强有力的合约，应用程序就可以快速改进和改变。 健康报告 停止逆向工程应用程序并开始从内部进行监控。 —— Kelsey Hightower，Monitorama PDX 2016：healthz 没有人比开发人员更了解应用程序需要什么才能以健康的状态运行。很长一段时间，基础设施管理员都试图从他们负责运行的应用程序中找出 “健康” 该怎么定义。如果不实际了解应用程序的健康状况，他们尝试在应用程序不健康时进行监控并发出警报，这往往是脆弱和不完整的。 为了提高云原生应用程序的可操作性，应用程序应该暴露健康检查。开发人员可以将其实施为命令或过程信号，以便应用程序在执行自我检查之后响应，或者更常见的是：通过应用程序提供 Web 服务，返回 HTTP 状态码来检查健康状态。 Google Borg 示例 Google 的 Borg 报告中列出了一个健康报告的例子： 几乎每个在 Borg 下运行的任务都包含一个内置的 HTTP 服务器，该服务器发布有关任务运行状况和数千个性能指标（如 RPC 延迟）的信息。Borg 会监控运行状况检查 URL 并重新启动不及时响应或返回 HTTP 错误代码的任务。其他数据由监控工具跟踪，用于仪表板和服务级别目标（SLO）违规警报。 将健康责任转移到应用程序中使应用程序更容易管理和自动化。应用程序应该知道它是否正常运行以及它依赖于什么（例如，访问数据库）来提供业务价值。这意味着开发人员需要与产品经理合作来定义应用服务的业务功能并相应地编写测试。 提供健康检查的应用程序示例包括 Zookeeper 的 ruok 命令和 etcd 的 HTTP / 健康端点。 应用程序不仅仅有健康或不健康的状态。它们将经历一个启动和关闭过程，在这个过程中它们应该通过健康检查，报告它们的状态。如果应用程序可以让平台准确了解它所处的状态，平台将更容易知道如何操作它。 一个很好的例子就是当平台需要知道应用程序何时可以接收流量。在应用程序启动时，如果它不能正确处理流量，它就应该表现为未准备好。此额外状态将防止应用程序过早终止，因为如果运行状况检查失败，平台可能会认为应用程序不健康，并且会反复停止或重新启动它。 应用程序健康只是能够自动化应用程序生命周期的一部分。除了知道应用程序是否健康之外，您还需要知道应用程序是否正在进行哪些工作。这些信息来自遥测数据。 遥测数据 遥测数据是进行决策所需的信息。确实，遥测数据可能与健康报告重叠，但它们有不同的用途。健康报告通知我们应用程序生命周期状态，而遥测数据通知我们应用程序业务目标。 您测量的指标有时称为服务级指标（SLI）或关键性能指标（KPI）。这些是特定于应用程序的数据，可以确保应用程序的性能处于服务级别目标（SLO）内。如果您需要更多关于这些术语的信息以及它们与您的应用程序、业务需求的关系，我们推荐你阅读来自 Site Reliability Engineering（O'Reilly）的第 4 章。 遥测和度量标准用于解决以下问题： 应用程序每分钟收到多少请求？ 有没有错误？ 什么是应用程序延迟？ 订购需要多长时间？ 通常会将数据刮取或推送到时间序列数据库（例如 Prometheus 或 InfluxDB）进行聚合。遥测数据的唯一要求是它将被收集数据的系统格式化。 至少，可能最好实施度量标准的 RED 方法，该方法收集应用程序的速率，错误和执行时间。 请求率 收到了多少个请求 错误 应用程序有多少错误 时间 多久才能收到回复 遥测数据应该用于提醒而非健康监测。在动态的、自我修复的环境中，我们更少关注单个应用程序实例的生命周期，更多关注关于整体应用程序 SLO 的内容。健康报告对于自动应用程序管理仍然很重要，但不应该用于页面工程师。 如果 1 个实例或 50 个应用程序不健康，只要满足应用程序的业务需求，我们可能不会收到警报。度量标准可让您知道您是否符合您的 SLO，应用程序的使用方式以及对于您的应用程序来说什么是 “正常”。警报有助于您将系统恢复到已知的良好状态。 警报也不应该与日志记录混淆。记录用于调试，开发和观察模式。它暴露了应用程序的内部功能。度量有时可以从日志（例如错误率）计算，但需要额外的聚合服务（例如 ElasticSearch）和处理。 公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:05 "},"cloud-native/kubernetes-and-cloud-native-app-overview.html":{"url":"cloud-native/kubernetes-and-cloud-native-app-overview.html","title":"Kubernetes 与云原生应用概览","keywords":"","body":"微服务介绍 微服务（Microservices）这个词比较新颖，但是其实这种架构设计理念早就有了。微服务是一种分布式架构设计理念，为了推动细粒度服务的使用，这些服务要能协同工作，每个服务都有自己的生命周期。一个微服务就是一个独立的实体，可以独立的部署在PAAS平台上，也可以作为一个独立的进程在主机中运行。服务之间通过API访问，修改一个服务不会影响其它服务。 云原生概念介绍 下面是Cloud Native概念思维导图 图 2.3.1：Cloud native思维导图 为了解决传统应用升级缓慢、架构臃肿、不能快速迭代、故障不能快速定位、问题无法快速解决等问题，云原生这一概念横空出世。 另外，云原生也很好地解释了云上运行的应用应该具备什么样的架构特性——敏捷性、可扩展性、故障可恢复性。 云原生应用应该具备以下几个关键词： 敏捷 可靠 高弹性 易扩展 故障隔离保护 不中断业务持续更新 Kubernetes与云原生的关系 Kuberentes可以说是乘着Docker和微服务的东风，一经推出便迅速蹿红，它的很多设计思想都契合了微服务和云原生应用的设计法则。 Kubernetes介绍 Kubernetes是Google基于Borg开源的容器编排调度引擎，作为CNCF（Cloud Native Computing Foundation）最重要的组件之一，它的目标不仅仅是一个编排系统，而是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes可以帮你将系统自动得达到和维持在这个状态。 更直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求Kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个Kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求Kubernetes API来改变应用程序的状态。 12因素应用 图 2.3.2：十二因素应用 1.基准代码 每个代码仓库（repo）都生成docker image保存到镜像仓库中，并使用唯一的ID管理，在Jenkins中使用编译时的ID。 2.依赖 显式的声明代码中的依赖，使用软件包管理工具声明，比如Go中的Glide。 3.配置 将配置与代码分离，应用部署到Kubernetes中可以使用容器的环境变量或ConfigMap挂载到容器中。 4.后端服务 把后端服务当作附加资源，实质上是计算存储分离和降低服务耦合，分解单体应用。 5.构建、发布、运行 严格分离构建和运行，每次修改代码生成新的镜像，重新发布，不能直接修改运行时的代码和配置。 6.进程 应用程序进程应该是无状态的，这意味着再次重启后还可以计算出原先的状态。 7.端口绑定 在Kubernetes中每个Pod都有独立的IP，每个运行在Pod中的应用不必关心端口是否重复，只需在service中指定端口，集群内的service通过配置互相发现。 8.并发 每个容器都是一个进程，通过增加容器的副本数实现并发。 9.易处理 快速启动和优雅终止可最大化健壮性，Kuberentes优秀的Pod生存周期控制。 10.开发环境与线上环境等价 在Kubernetes中可以创建多个namespace，使用相同的镜像可以很方便的复制一套环境出来，镜像的使用可以很方便的部署一个后端服务。 11.日志 把日志当作事件流，使用stdout输出并收集汇聚起来，例如到ES中统一查看。 12.管理进程 后台管理任务当作一次性进程运行，kubectl exec进入容器内部操作。 容器的设计模式 Kubernetes提供了多种资源对象，用户可以根据自己应用的特性加以选择。这些对象有： 类别 名称 资源对象 Pod、ReplicaSet、ReplicationController、Deployment、StatefulSet、DaemonSet、Job、CronJob、HorizontalPodAutoscaler 配置对象 Node、Namespace、Service、Secret、ConfigMap、Ingress、Label、CustomResourceDefinition、 ServiceAccount 存储对象 Volume、Persistent Volume 策略对象 SecurityContext、ResourceQuota、LimitRange 资源限制与配额 两层的资源限制与配置 Pod级别，最小的资源调度单位 Namespace级别，限制资源配额和每个Pod的资源使用区间 部署Kubernetes集群 使用二进制部署 kubernetes 集群的所有组件和插件。 集群详情 Kubernetes 1.6.0 Docker 1.12.5（使用yum安装） Etcd 3.1.5 Flanneld 0.7 vxlan 网络 TLS 认证通信 (所有组件，如 etcd、kubernetes master 和 node) RBAC 授权 kubelet TLS BootStrapping kubedns、dashboard、heapster(influxdb、grafana)、EFK(elasticsearch、fluentd、kibana) 集群插件 私有Docker镜像仓库Harbor 步骤介绍 创建 TLS 证书和秘钥 创建kubeconfig文件 创建高可用etcd集群 安装kubectl命令行工具 部署master节点 安装flannel网络插件 部署node节点 安装kubedns插件 安装dashboard插件 安装heapster插件 安装EFK插件 服务发现与负载均衡 Service：直接用Service提供cluster内部的负载均衡，并借助cloud provider提供的LB提供外部访问 Ingress：还是用Service提供cluster内部的负载均衡，但是通过自定义LB提供外部访问 Service Load Balancer：把load balancer直接跑在容器中，实现Bare Metal的Service Load Balancer Custom Load Balancer：自定义负载均衡，并替代kube-proxy，一般在物理部署Kubernetes时使用，方便接入公司已有的外部服务 持续集成与发布 图 2.3.3：使用Jenkins进行持续集成与发布流程图 应用构建和发布流程说明： 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成Docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的Kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的Kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在Ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。 Jenkins调用Kubernetes的API，部署应用 日志收集与监控 ，选用filebeat来收集日志。 图 2.3.4：filebeat日志收集架构图 安全性与权限管理 Kubernetes是一个多租户的云平台，因此必须对用户的权限加以限制，对用户空间进行隔离。Kubernetes中的隔离主要包括这几种： 网络隔离：需要使用网络插件，比如flannel, calico。 资源隔离：kubernetes原生支持资源隔离，pod就是资源隔离和调度的最小单位，同时使用namespace限制用户空间和资源限额。 身份隔离：使用RBAC-基于角色的访问控制，多租户的身份认证和权限控制。 DevOps 下面是社区中Kubernetes开源爱好者的分享内容，我觉得是对Kubernetes在DevOps中应用的很好的形式值得大家借鉴。 真正践行DevOps，让开发人员在掌握自己的开发和测试环境，让环境一致，让开发效率提升，让运维没有堆积如山的tickets，让监控更加精准，从Kubernetes平台开始。 行动指南 根据环境（比如开发、测试、生产）划分namespace，也可以根据项目来划分 再为每个用户划分一个namespace、创建一个serviceaccount和kubeconfig文件，不同namespace间的资源隔离，目前不隔离网络，不同namespace间的服务可以互相访问 创建yaml模板，降低编写Kubernetes yaml文件编写难度 在kubectl命令上再封装一层，增加用户身份设置和环境初始化操作，简化kubectl命令和常用功能 管理员通过dashboard查看不同namespace的状态，也可以使用它来使操作更便捷 所有应用的日志统一收集到ElasticSearch中，统一日志访问入口 可以通过Grafana查看所有namespace中的应用的状态和kubernetes集群本身的状态 需要持久化的数据保存在分布式存储中，例如GlusterFS或Ceph中 使用Kibana查看日志 日志字段中包括了应用的标签、容器名称、主机名称、宿主机名称、IP地址、时间。 图 2.3.5：kibana界面 使用Grafana查看应用状态 监控分类示意图： 图 2.3.6：Grafana界面示意图1 Kubernetes集群全局监控图1 该监控图可以看到集群硬件使用情况。 图 2.3.7：Grafana界面示意图2 Kubernetes全局监控图2 该监控可以看到单个用户的namespace下的所有资源的使用情况。 图 2.3.8：Grafana界面示意图3 公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:05 "},"cloud-native/from-kubernetes-to-cloud-native.html":{"url":"cloud-native/from-kubernetes-to-cloud-native.html","title":"Kubernetes 到 Cloud Native","keywords":"","body":"云原生应用之路——从Kubernetes到Cloud Native 为什么使用Kubernetes Kubernetes——让容器应用进入大规模工业生产。 Kubernetes是容器编排系统的事实标准 在单机上运行容器，无法发挥它的最大效能，只有形成集群，才能最大程度发挥容器的良好隔离、资源分配与编排管理的优势，而对于容器的编排管理，Swarm、Mesos和Kubernetes的大战已经基本宣告结束，Kubernetes成为了无可争议的赢家。 下面这张图是Kubernetes的架构图，其中显示了组件之间交互的接口CNI、CRI、OCI等，这些将Kubernetes与某款具体产品解耦，给用户最大的定制程度，使得Kubernetes有机会成为跨云的真正的云原生应用的操作系统。 图 2.4.1：Kubernetes架构 云原生的核心目标 图 2.4.2：Cloud Native Core target Cloud Native DevOps——通向云原生的云梯 CNCF（云原生计算基金会）给出了云原生应用的三大特征： 容器化包装：软件应用的进程应该包装在容器中独立运行。 动态管理：通过集中式的编排调度系统来动态的管理和调度。 微服务化：明确服务间的依赖，互相解耦。 云原生所需要的能力和特征。 图 2.4.3：Cloud Native Features 使用Kubernetes构建云原生应用 我们都是知道Heroku推出了适用于PaaS的12 factor app的规范，包括如下要素： 基准代码 依赖管理 配置 后端服务 构建，发布，运行 无状态进程 端口绑定 并发 易处理 开发环境与线上环境等价 日志作为事件流 管理进程 API声明管理 认证和授权 监控与告警 使用Kubernetes构建云原生架构： 图 2.4.4：Building a Cloud Native Architecture with Kubernetes followed 12 factor app 使用场景 Cloud Native的大规模工业生产 Open Source Kubernetes调研方案选择。 图 2.4.5：Kubernetes solutions 公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:05 "},"concepts/time.html":{"url":"concepts/time.html","title":"Kubernetes 架构","keywords":"","body":"公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:05 "},"practice/time.html":{"url":"practice/time.html","title":"在 CentOS 上部署 Kubernetes 集群","keywords":"","body":"公司信息Copyright © 2017-2020 | Distributed under all right reserved，powered by Gitbook Updated at 2021-02-04 15:06:07 "}}